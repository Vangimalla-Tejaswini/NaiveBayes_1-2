{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f5721c",
   "metadata": {},
   "source": [
    "#### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3bf19c",
   "metadata": {},
   "source": [
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use conditional probability.\n",
    "\n",
    "Let's denote:\n",
    "\n",
    "A: Event that an employee uses the health insurance plan.\n",
    "    \n",
    "B: Event that an employee is a smoker.\n",
    "    \n",
    "We're given:\n",
    "\n",
    "P(A) = 0.70 (probability that an employee uses the health insurance plan)\n",
    "\n",
    "P(B|A) = 0.40 (probability that an employee is a smoker given that they use the health insurance plan)\n",
    "\n",
    "We want to find P(B|A), the probability that an employee is a smoker given that they use the health insurance plan, which is the conditional probability of B given A.\n",
    "\n",
    "Using the conditional probability formula:\n",
    "    \n",
    "P(B∣A)= P(A∩B)/P(A)\n",
    "\n",
    "We can rearrange the formula to solve for P(A∩B)=P(B∣A)×P(A)\n",
    "\n",
    "Substituting the given values:\n",
    "\n",
    "    P(A∩B)=0.40×0.70=0.28\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.28, or 28%.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a18dde",
   "metadata": {},
   "source": [
    "#### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa9a5f5",
   "metadata": {},
   "source": [
    "#### Bernoulli Naive Bayes\n",
    "#### Assumption about Data: \n",
    "            It assumes that all features are binary such that they take only two values (e.g., 0 or 1). A common example is text classification where each word in the vocabulary is either present or absent in a document.\n",
    "#### Use Case:\n",
    "           It's particularly useful for binary/bag of words count features.\n",
    "#### Mathematical Model:\n",
    "           The probability of each feature belonging to a certain class is modeled using a Bernoulli distribution. Multinomial \n",
    "#### Naive Bayes\n",
    "#### Assumption about Data:\n",
    "           It assumes that features represent the frequencies with which certain events have been generated by a multinomial distribution. This is the case where each feature counts something, such as the number of times a word appears in a document.\n",
    "#### Use Case:\n",
    "           It's used for discrete data and is particularly suited for count-based feature vectors (e.g., word counts in text classification).\n",
    "#### Mathematical Model: \n",
    "           The probability of observing a sampled vector is given by a multinomial distribution, reflecting the count of features (e.g., word counts).\n",
    "           \n",
    "           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d0d7e",
   "metadata": {},
   "source": [
    "#### Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e8219",
   "metadata": {},
   "source": [
    "In the Bernoulli Naive Bayes model, each feature of the input vector represents the presence or absence of something (e.g., a word in text classification). The feature vector for a document (or any instance) is a binary vector where:\n",
    "\n",
    "1 indicates the presence of the feature (e.g., a word is present in the document),\n",
    "\n",
    "0 indicates the absence of the feature (e.g., a word is not present in the document).\n",
    "#### Handling \"Missing\" Features\n",
    "\n",
    "In the context of Bernoulli Naive Bayes, a \"missing\" feature is essentially treated as an absence (0). This model is based on binary outcomes for each feature; hence, if a feature (word) does not occur in a document, it is naturally treated as absent. This is a critical part of the model's design and not an additional mechanism to handle missing data.\n",
    "\n",
    "#### Example\n",
    "If your vocabulary includes the words [\"love\", \"action\", \"comedy\", \"drama\"] and you're classifying movie reviews, a review that mentions only \"comedy\" and \"drama\" would be represented as [0, 0, 1, 1]. The words \"love\" and \"action\" are considered absent for this review. There's no need to \"handle\" the fact that \"love\" and \"action\" weren't mentioned; their absence is informative on its own and is directly used in calculating the probabilities for classification.\n",
    "\n",
    "#### Traditional Missing Values\n",
    "In more conventional scenarios where you might have missing values (e.g., unknown attributes in a dataset), the strategy for dealing with these would differ based on the model and situation:\n",
    "\n",
    "Some models might ignore features with missing values during training or prediction.\n",
    "Others might require you to impute missing values (fill them in with estimated or average values) before training or classification.\n",
    "\n",
    "However, for Bernoulli Naive Bayes, the concept of a missing value as it's traditionally understood (e.g., an unknown measurement or an empty field in a dataset) doesn't directly apply. The model's binary nature automatically incorporates the absence of features as part of its normal operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29910d7b",
   "metadata": {},
   "source": [
    "#### Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352ee06",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. The model is quite flexible and not limited to binary classification tasks. In a multi-class setting, Gaussian Naive Bayes assumes that the continuous values associated with each feature are distributed according to a Gaussian (normal) distribution for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c58b0",
   "metadata": {},
   "source": [
    "#### Q5. Assignment:\n",
    "#### Data preparation:\n",
    "#### Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "#### Implementation:\n",
    "#### Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "#### scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "#### dataset. You should use the default hyperparameters for each classifier.\n",
    "#### Results:\n",
    "#### Report the following performance metrics for each classifier:\n",
    "#### Accuracy\n",
    "#### Precision\n",
    "#### Recall\n",
    "#### F1 score\n",
    "##### Discussion:\n",
    "#### Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "#### Conclusion:\n",
    "#### Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "#### Note: This dataset contains a binary classification problem with multiple features. The dataset is relatively small, but it can be used to demonstrate the performance of the different variants of NaiveBayes on a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b32c1066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics:\n",
      "\n",
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8839\n",
      "Precision: 0.8870\n",
      "Recall: 0.8152\n",
      "F1 Score: 0.8481\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.7863\n",
      "Precision: 0.7393\n",
      "Recall: 0.7215\n",
      "F1 Score: 0.7283\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.8218\n",
      "Precision: 0.7104\n",
      "Recall: 0.9570\n",
      "F1 Score: 0.8131\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "names = [\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n",
    "         \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n",
    "         \"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n",
    "         \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\n",
    "         \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\",\n",
    "         \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\",\n",
    "         \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \"word_freq_technology\",\n",
    "         \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\", \"word_freq_cs\",\n",
    "         \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\", \"word_freq_re\", \"word_freq_edu\",\n",
    "         \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\", \"char_freq_[\", \"char_freq_!\",\n",
    "         \"char_freq_$\", \"char_freq_hash\", \"capital_run_length_average\", \"capital_run_length_longest\",\n",
    "         \"capital_run_length_total\", \"class\"]\n",
    "data = pd.read_csv(url, names=names, header=None)\n",
    "\n",
    "# Prepare data\n",
    "X = data.drop('class', axis=1)\n",
    "y = data['class']\n",
    "\n",
    "# Initialize classifiers\n",
    "models = {'Bernoulli Naive Bayes': BernoulliNB(),\n",
    "          'Multinomial Naive Bayes': MultinomialNB(),\n",
    "          'Gaussian Naive Bayes': GaussianNB()}\n",
    "\n",
    "# Evaluate each classifier\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    accuracy = cross_val_score(model, X, y, cv=10, scoring='accuracy')\n",
    "    precision = cross_val_score(model, X, y, cv=10, scoring='precision')\n",
    "    recall = cross_val_score(model, X, y, cv=10, scoring='recall')\n",
    "    f1 = cross_val_score(model, X, y, cv=10, scoring='f1')\n",
    "\n",
    "    results[name] = {'Accuracy': np.mean(accuracy),\n",
    "                     'Precision': np.mean(precision),\n",
    "                     'Recall': np.mean(recall),\n",
    "                     'F1 Score': np.mean(f1)}\n",
    "\n",
    "# Print results\n",
    "print(\"Performance Metrics:\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, score in metrics.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23592d59",
   "metadata": {},
   "source": [
    "##### Bernoulli Naive Bayes performed the best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da2e6f",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes performed the best among the three variants, achieving the highest accuracy, precision, and F1 score.\n",
    "Multinomial Naive Bayes had slightly lower performance compared to Bernoulli Naive Bayes.\n",
    "\n",
    "Gaussian Naive Bayes showed high recall but lower precision, indicating it may have misclassified some non-spam messages as spam.\n",
    "\n",
    "Suggestions for future work include feature engineering, trying different classification algorithms, parameter tuning, using ensemble methods, data augmentation, handling class imbalance, and exploring alternative cross-validation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d243b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
